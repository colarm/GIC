{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification-WORKSHOP NOTES.ipynb","provenance":[{"file_id":"1Ws9_XE28ymNOrtvWz9gcr2WaNcSdIglz","timestamp":1574347493624}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tFg9I2KjVxbf","colab_type":"text"},"source":["---\n","# Crash Course Python for Data Science — Predictive Modelling\n","---\n","# 02 - Classification modelling\n","---"]},{"cell_type":"markdown","metadata":{"id":"L_1l5LBdVxbg","colab_type":"text"},"source":["Previously:\n","\n","#### [regression model](https://developers.google.com/machine-learning/glossary/#regression_model)\n","> A type of model that outputs **continuous (typically, floating-point) values.**\n","\n","Today: \n","\n","#### [classification model](https://developers.google.com/machine-learning/glossary/#classification_model)\n","> A type of machine learning model for distinguishing among **two or more discrete classes.**\n","\n","Both are types of:\n","\n","#### [supervised machine learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)\n","> Training a model from input data and its corresponding labels. **Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers.** After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with unsupervised machine learning.\n","\n","For examples, see this [**diagram by Andrew Ng in Harvard Business Review**](https://hbr.org/resources/images/article_assets/2016/10/W161026_NG_WHATMACHINEv2.png)."]},{"cell_type":"markdown","metadata":{"id":"5ZT-YOnrVxbh","colab_type":"text"},"source":["## Linear Regression vs Logistic Regression, for Classification"]},{"cell_type":"code","metadata":{"id":"SjDSr8ZvVxbh","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification\n","# plot tunning\n","plt.style.use(style='ggplot')\n","plt.rcParams['figure.figsize'] = (10, 6)\n","\n","# Generate data \n","x, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, \n","                           n_classes=2, n_clusters_per_class=1, class_sep=0.25, random_state=0)\n","\n","plt.style.use('fivethirtyeight')\n","plt.scatter(x, y, s=50, alpha=0.5)\n","plt.xlabel('feature')\n","plt.ylabel('target');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9FXm8iwVxbl","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","model.fit(x, y)\n","\n","inputs = [[x/100] for x in range(-100, 100)]\n","predictions = model.predict(inputs)\n","\n","plt.scatter(x, y, s=50, alpha=0.5)\n","plt.plot(inputs, predictions)\n","plt.xlabel('feature')\n","plt.ylabel('target')\n","plt.title('Linear Regression');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_-2waCgVxbp","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression()\n","\n","model.fit(x, y)\n","predictions = model.predict(inputs)\n","\n","plt.scatter(x, y, s=100, alpha=0.5);\n","plt.plot(inputs, predictions)\n","plt.xlabel('feature')\n","plt.ylabel('target')\n","plt.title('Logistic Regression');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqC6cz0RVxbs","colab_type":"code","colab":{}},"source":["predictions = model.predict_proba(inputs)[:, 1]\n","\n","plt.scatter(x, y, s=100, alpha=0.5)\n","plt.plot(inputs, predictions)\n","plt.xlabel('feature')\n","plt.ylabel('target')\n","plt.title('Logistic Regression probabilities');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSlj46yQVxbv","colab_type":"text"},"source":["# Predict survival on the Titanic\n","\n","## Load data"]},{"cell_type":"code","metadata":{"id":"EocXLirRVxbv","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","train, test = train_test_split(sns.load_dataset('titanic'), random_state=0)\n","train.shape, test.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IV3y9G_sVxby","colab_type":"code","colab":{}},"source":["train.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2udmYDsgVxb1","colab_type":"code","colab":{}},"source":["train.survived.value_counts(normalize=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ri7ViZOmVxb3","colab_type":"text"},"source":["## Majority classifier"]},{"cell_type":"markdown","metadata":{"id":"LyqW1JR4Vxb3","colab_type":"text"},"source":["[*Data Science for Business*](https://www.safaribooksonline.com/library/view/data-science-for/9781449374273/) recommends,\n","\n","> For classification tasks, one good baseline is the ***majority classifier, a naive classifier that always chooses the majority class of the training dataset.*** ... \n","\n","> This may seem like advice so obvious it can be passed over quickly, but it is worth spending an extra moment here. There are many cases where smart, analytical people have been tripped up in skipping over this basic comparison. For example, an analyst may see a classification accuracy of 94% from her classifier and conclude that it is doing fairly well—when in fact only 6% of the instances are positive. So, the simple majority prediction classifier also would have an accuracy of 94%."]},{"cell_type":"code","metadata":{"id":"pnzB7AWJVxb4","colab_type":"code","colab":{}},"source":["predictions = [0] * len(train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWVdwCRtVxb6","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(train.survived, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQ9yuhFcVxb8","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","\n","def matrix(y_true, y_pred):\n","    data = confusion_matrix(y_true, y_pred)\n","    index = ['Actual 0', 'Actual 1']\n","    columns = ['Predicted 0', 'Predicted 1']\n","    return pd.DataFrame(data, index, columns)\n","                        \n","matrix(train.survived, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIm2HYbVVxb_","colab_type":"code","colab":{}},"source":["from sklearn.metrics import recall_score\n","recall_score(train.survived, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLsVjCe_VxcA","colab_type":"code","colab":{}},"source":["# Recall\n","0 / 258"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY_q39frVxcC","colab_type":"code","colab":{}},"source":["# Accuracy\n","410 / (410 + 258)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QeCpYNFrVxcF","colab_type":"text"},"source":["# Feature preprocessing"]},{"cell_type":"code","metadata":{"id":"SjzoY_QnVxcG","colab_type":"code","colab":{}},"source":["train[['sex', 'age']].info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1C42H9tVxcK","colab_type":"text"},"source":["### Encode `sex`"]},{"cell_type":"code","metadata":{"id":"IiUOiRwKVxcL","colab_type":"code","colab":{}},"source":["train['female'] = train.sex == 'female'\n","test['female'] = test.sex == 'female'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrfHu7okVxcN","colab_type":"code","colab":{}},"source":["train[['sex', 'female']].head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXjQl6uAVxcP","colab_type":"text"},"source":["### Impute `age`"]},{"cell_type":"code","metadata":{"id":"Anp6qMaYVxcQ","colab_type":"code","colab":{}},"source":["train.age.isnull().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGrTXkpqVxcT","colab_type":"code","colab":{}},"source":["train.age.mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3t0ItZ4VxcV","colab_type":"code","colab":{}},"source":["train.age.fillna(train.age.mean(), inplace=True)\n","train.age.isnull().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6xEPiJ2VxcW","colab_type":"code","colab":{}},"source":["test.age.isnull().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3EMgbHBVxcY","colab_type":"code","colab":{}},"source":["test.age.mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hiq2Yn1oVxca","colab_type":"code","colab":{}},"source":["test.age.fillna(train.age.mean(), inplace=True)\n","test.age.isnull().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tVhZdvSVxcb","colab_type":"text"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"id":"iimVUF_pVxcb","colab_type":"code","colab":{}},"source":["features = ['female', 'age']\n","target = 'survived'\n","\n","model = LogisticRegression()\n","model.fit(train[features], train[target])\n","\n","# Train accuracy\n","y_true = train[target]\n","y_pred = model.predict(train[features])\n","print('Train accuracy:', accuracy_score(y_true, y_pred))\n","\n","# Test accuracy\n","y_true = test[target]\n","y_pred = model.predict(test[features])\n","print('Test accuracy:', accuracy_score(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mj87DLqVxce","colab_type":"code","colab":{}},"source":["matrix(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zloYkNWiVxcf","colab_type":"code","colab":{}},"source":["recall_score(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUTk4mKcVxci","colab_type":"code","colab":{}},"source":["# Recall\n","59 / (59 + 25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HneX0YHqVxcj","colab_type":"code","colab":{}},"source":["# Accuracy\n","(59 + 115) / (59 + 115 + 24 + 25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCDWlQeVVxcl","colab_type":"code","colab":{}},"source":["model.coef_, model.intercept_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuzTLyZiVxco","colab_type":"code","colab":{}},"source":["pd.Series(model.coef_[0], features).plot.barh(color='gray')\n","plt.title('Logistic Regression coefficients');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjs0oMQuVxcp","colab_type":"text"},"source":["# Decision Tree"]},{"cell_type":"code","metadata":{"id":"lVZYhYCEVxcq","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeClassifier\n","model = DecisionTreeClassifier(max_depth=2)\n","model.fit(train[features], train[target])\n","\n","# Train accuracy\n","y_true = train[target]\n","y_pred = model.predict(train[features])\n","print('Train accuracy:', accuracy_score(y_true, y_pred))\n","\n","# Test accuracy\n","y_true = test[target]\n","y_pred = model.predict(test[features])\n","print('Test accuracy:', accuracy_score(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0Gsb6FTVxct","colab_type":"code","colab":{}},"source":["recall_score(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJqtezqqVxcv","colab_type":"code","colab":{}},"source":["matrix(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNr7Y0a7Vxcx","colab_type":"code","colab":{}},"source":["pd.Series(model.feature_importances_, features).plot.barh(color='gray')\n","plt.title('Decision Tree feature importances');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_ivlt_iVxcz","colab_type":"text"},"source":["### Visualize the tree\n","\n","#### To install Graphviz\n","- On Google Colab, run the cell below.\n","- On your local machine, I recommend you use [Anaconda](https://www.anaconda.com/download/) instead: `conda install python-graphviz`"]},{"cell_type":"code","metadata":{"id":"oB06WMrOVxcz","colab_type":"code","colab":{}},"source":["!pip install graphviz\n","!apt-get install graphviz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3udRBTt-Vxc0","colab_type":"code","colab":{}},"source":["import graphviz\n","from sklearn.tree import export_graphviz\n","\n","dot_data = export_graphviz(model, out_file=None, feature_names=features, \n","                           class_names=['Died', 'Survived'], \n","                           filled=True, impurity=False, rotate=True)\n","graphviz.Source(dot_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BY0U2dXUi1RJ","colab_type":"text"},"source":["### Another visualization\n","\n","#### Install `dtreeviz` and dependencies"]},{"cell_type":"code","metadata":{"id":"rQheNlMViyWa","colab_type":"code","colab":{}},"source":["!pip install dtreeviz\n","!apt install graphviz\n","!apt install xdg-utils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSqrVEQmjBNq","colab_type":"code","colab":{}},"source":["from dtreeviz.trees import *\n","\n","dtreeviz(model,\n","         train[features],\n","         train[target],\n","         target_name=target,\n","         feature_names=features, \n","         class_names=['Deceased', 'Survived']) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suut5w2XVxc8","colab_type":"text"},"source":["# Begin with baseline models\n","\n","> ***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n","\n","> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n","\n","> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n","\n","This advice is echoed and elaborated in several sources:\n","\n","#### [Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n","\n","> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n","\n","#### [Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n","\n","> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n","\n","#### [*Data Science for Business*](https://www.safaribooksonline.com/library/view/data-science-for/9781449374273/), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n","\n","> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n"]}]}