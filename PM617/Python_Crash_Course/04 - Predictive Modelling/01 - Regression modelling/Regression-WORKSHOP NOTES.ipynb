{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regression-WORKSHOP NOTES.ipynb","provenance":[{"file_id":"1yg75YgfyeKB1YU9w7ju6Lb_1pGl__YZq","timestamp":1574277864518}],"collapsed_sections":["j9MSyoC2l_ZR","bRyCX_j-l_ZR","idG-287bl_ZT","UYamuAxgl_ZT"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HEK9p8mhl_Xc","colab_type":"text"},"source":["---\n","# Crash Course Python for Data Science â€” Predictive Modelling\n","---\n","# 01 - Regression modelling\n","---\n","#### Lots of models!\n","- Nearest Neighbors\n","- Linear Regression\n","- Polynomial Regression\n","- \"Dummy\" Baseline\n","- Decision Trees\n","\n","#### Cool features!\n","- Models with one independent variable\n","- Models with multiple independent variables\n","- Encoding ordinal variables from strings to numbers\n","\n","#### Some evaluation!\n","- Overfitting vs Underfitting\n","- Train / Test split\n","- Mean Absolute Error\n","\n","#### Less linear algebra!\n","- None ðŸ˜‚\n","- Just `sklearn`"]},{"cell_type":"markdown","metadata":{"id":"D-2-uVrNl_Xd","colab_type":"text"},"source":["# Let's look at diamond data"]},{"cell_type":"code","metadata":{"id":"dYgLxEQFl_Xe","colab_type":"code","colab":{}},"source":["# %matplotlib inline will make your plot outputs appear \n","# and be stored within the notebook. It's known as a \"magic function\"\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# plot tunning\n","plt.style.use(style='ggplot')\n","plt.rcParams['figure.figsize'] = (10, 6)\n","\n","columns = ['carat', 'cut', 'price']\n","\n","# Just 10 data points\n","data = [[0.3, 'Ideal', 422],\n","        [0.31, 'Ideal', 489],\n","        [0.42, 'Premium', 737],\n","        [0.5, 'Ideal', 1415],\n","        [0.51, 'Premium', 1177],\n","        [0.7, 'Fair', 1865],\n","        [0.73, 'Fair', 2351],\n","        [1.01, 'Good', 3768],\n","        [1.18, 'Very Good', 3965],\n","        [1.18, 'Ideal', 4838]]\n","\n","train = pd.DataFrame(data=data, columns=columns)\n","\n","plt.style.use('fivethirtyeight')\n","train.plot.scatter(x='carat', y='price', s=50);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zuppE_Hl_Xi","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ki9XWxWFl_Xk","colab_type":"text"},"source":["# Nearest Neighbors"]},{"cell_type":"code","metadata":{"id":"fMOb5DVrl_Xm","colab_type":"code","colab":{}},"source":["features = ['carat']\n","target = 'price'\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","model = KNeighborsRegressor(n_neighbors=2)\n","\n","model.fit(train[features], train[target])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0skpUf5o58WZ","colab_type":"text"},"source":["Predict price of 0.7 carat diamond"]},{"cell_type":"code","metadata":{"id":"jRMn_Mspl_Xo","colab_type":"code","colab":{}},"source":["model.predict([[0.7]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hhxoRdUx6BBG","colab_type":"text"},"source":["Predict price of 0.9 carat diamond"]},{"cell_type":"code","metadata":{"id":"zVnk0J52l_Xq","colab_type":"code","colab":{}},"source":["model.predict([[0.9]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qA5n4ATt6D3c","colab_type":"text"},"source":["Predict price of 1.2 carat diamond"]},{"cell_type":"code","metadata":{"id":"Ym1oPIfHl_Xt","colab_type":"code","colab":{}},"source":["model.predict([[1.2]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7eKP5UP6H6h","colab_type":"text"},"source":["We can get multiple predictions at the same time"]},{"cell_type":"code","metadata":{"id":"vYcpnX46l_Xv","colab_type":"code","colab":{}},"source":["model.predict([[0.7], [0.9], [1.2]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OznZvYcA6Qby","colab_type":"text"},"source":["We can get predictions for each 1/100 carat, from 0 to 1.3 carats"]},{"cell_type":"code","metadata":{"id":"iq194oi5l_Xy","colab_type":"code","colab":{}},"source":["# [[0.0], [0.01], [0.02], ... [1.28], [1.29]]\n","carats = [[c/100] for c in range(130)]\n","predictions = model.predict(carats)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRZBE3VI6wdd","colab_type":"text"},"source":["Plot these predictions"]},{"cell_type":"code","metadata":{"id":"RNTD4LlWl_X2","colab_type":"code","colab":{}},"source":["train.plot.scatter(x='carat', y='price', s=50)\n","plt.step(carats, predictions)\n","plt.title('Nearest Neighbors Model');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4M2Vb4uWl_X5","colab_type":"text"},"source":["# Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"5HHkR-2p66i6","colab_type":"text"},"source":["For a Linear Regression, the first two lines of code are different, but the rest of the cell is the same."]},{"cell_type":"code","metadata":{"id":"FxVkvr3rl_X5","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","\n","model.fit(train[features], train[target])\n","predictions = model.predict(carats)\n","train.plot.scatter(x='carat', y='price', s=50)\n","plt.plot(carats, predictions)\n","plt.title('Linear Regression');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vq6MpegG7HWU","colab_type":"text"},"source":["We can get the equation for the line:"]},{"cell_type":"code","metadata":{"id":"kkg5pyOSl_X9","colab_type":"code","colab":{}},"source":["model.coef_, model.intercept_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_gWfONSl_X9","colab_type":"text"},"source":["$$ y = mx + b $$\n","\n","$$ price = 4,627*carat - 1,062 $$"]},{"cell_type":"markdown","metadata":{"id":"TnV5Ezb1l_X_","colab_type":"text"},"source":["# Polynomial Regression"]},{"cell_type":"markdown","metadata":{"id":"pD9WC8kw7RSh","colab_type":"text"},"source":["Let's try another model. Again, the first few lines of code are different, but the rest of the cell is the same.\n","\n","This model is more flexible â€” *too* flexible. It \"overfits.\""]},{"cell_type":"code","metadata":{"id":"zN53lhMOl_X_","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","model = make_pipeline(PolynomialFeatures(degree=8), LinearRegression())\n","\n","model.fit(train[features], train[target])\n","predictions = model.predict(carats)\n","train.plot.scatter(x='carat', y='price', s=50)\n","plt.plot(carats, predictions)\n","plt.title('8th Degree Polynomial (Overfit)')\n","plt.ylim((-1000, 5000));"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opjS-eD670Af","colab_type":"text"},"source":["This model fits the training data almost perfectly, but doesn't generalize to new data, so some predictions are absurd:"]},{"cell_type":"code","metadata":{"id":"BXtOoQNHl_YB","colab_type":"code","colab":{}},"source":["model.predict([[0.9]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKeP0v3k8bIl","colab_type":"text"},"source":["If you're curious, here are the polynomial coefficients. The model is overly complex."]},{"cell_type":"code","metadata":{"id":"x10Ys963l_YF","colab_type":"code","colab":{}},"source":["# Don't worry about understanding this code\n","print('Intercept', model.named_steps['linearregression'].intercept_.astype(int))\n","print(pd.Series(model.named_steps['linearregression'].coef_.astype(int), \n","                model.named_steps['polynomialfeatures'].get_feature_names('c')))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNtWnaHil_YH","colab_type":"text"},"source":["# \"Dummy\" Baseline"]},{"cell_type":"markdown","metadata":{"id":"hj-9sCEf8lBn","colab_type":"text"},"source":["This model has the opposite problem â€” it \"underfits.\""]},{"cell_type":"code","metadata":{"id":"iw0aYcbul_YH","colab_type":"code","colab":{}},"source":["from sklearn.dummy import DummyRegressor\n","model = DummyRegressor(strategy='mean')\n","\n","model.fit(train[features], train[target])\n","predictions = model.predict(carats)\n","train.plot.scatter(x='carat', y='price', s=50)\n","plt.plot(carats, predictions)\n","plt.title('Mean Baseline (Underfit)');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6N8n4l-Wl_YJ","colab_type":"text"},"source":["# Test data"]},{"cell_type":"markdown","metadata":{"id":"hKUrktie8rR3","colab_type":"text"},"source":["With so many options, how do we choose the best model?\n","\n","To avoid overfitting, use different data for training and testing."]},{"cell_type":"code","metadata":{"id":"MaTEMhM9l_YK","colab_type":"code","colab":{}},"source":["data = [[0.3, 'Ideal', 432],\n","        [0.34, 'Ideal', 687],\n","        [0.37, 'Premium', 1124],\n","        [0.4, 'Good', 720],\n","        [0.51, 'Ideal', 1397],\n","        [0.51, 'Very Good', 1284],\n","        [0.59, 'Ideal', 1437],\n","        [0.7, 'Ideal', 3419],\n","        [0.9, 'Premium', 3484],\n","        [0.9, 'Fair', 2964]]\n","\n","test = pd.DataFrame(data=data, columns=columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2vm4ulsl_YL","colab_type":"code","colab":{}},"source":["ax = train.plot.scatter(x='carat', y='price', s=50)\n","test.plot.scatter(x='carat', y='price', s=50, color='orange', ax=ax)\n","plt.legend(['Train data', 'Test data']);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79s18-Ull_YO","colab_type":"text"},"source":["# Compare four models\n","\n","How well does each model fit the training data? The test data?"]},{"cell_type":"code","metadata":{"id":"bePXZBtDl_YP","colab_type":"code","colab":{}},"source":["# Use mean absolute error to compare models\n","from sklearn.metrics import mean_absolute_error\n","\n","# Compare four models\n","models = [('Nearest Neighbors Model', KNeighborsRegressor(n_neighbors=2)),\n","          ('Linear Regression', LinearRegression()),\n","          ('8th Degree Polynomial', make_pipeline(PolynomialFeatures(degree=8), LinearRegression())),\n","          ('Mean Baseline', DummyRegressor(strategy='mean'))]\n","\n","for name, model in models:\n","    \n","    # Plot training data (blue dots)\n","    ax = train.plot.scatter(x='carat', y='price', s=50)\n","    \n","    # Plot testing data (orange dots)\n","    test.plot.scatter(x='carat', y='price', s=50, color='orange', ax=ax)\n","    \n","    # Train model\n","    model.fit(train[features], train[target])\n","    \n","    # Plot predictions (blue line)\n","    plt.plot(carats, model.predict(carats))\n","    plt.title(name)\n","    plt.ylim((-1000, 5000))\n","    plt.show()\n","    \n","    # Calculate mean absolute error on the training data\n","    # (average distance between line and blue dots)\n","    y_true = train[target]\n","    y_pred = model.predict(train[features])\n","    train_error = mean_absolute_error(y_true, y_pred)\n","    \n","    # Calculate mean absolute error on the test data\n","    # (average distance between line and orange dots)\n","    y_true = test[target]\n","    y_pred = model.predict(test[features])\n","    test_error = mean_absolute_error(y_true, y_pred)\n","    \n","    # Display the errors\n","    print('Train Error: $', round(train_error))\n","    print('Test Error: $', round(test_error), '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quT0eDz8l_YR","colab_type":"text"},"source":["# Mean absolute error\n","\n","The first step is to calculate the difference between predicted values and true values. "]},{"cell_type":"code","metadata":{"id":"uAQXwWu1l_YR","colab_type":"code","colab":{}},"source":["# Error for Baseline Model\n","predicted = train.price.mean()\n","true = train.price\n","(predicted - true)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgCXrJzA98-Y","colab_type":"text"},"source":["Then take the absolute value"]},{"cell_type":"code","metadata":{"id":"FpBmBW6Wl_YT","colab_type":"code","colab":{}},"source":["# Absolute Error for Baseline Model\n","(predicted - true).abs()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BccIHAzX-Ap2","colab_type":"text"},"source":["And then the mean."]},{"cell_type":"code","metadata":{"id":"UoX4rhUPl_YV","colab_type":"code","colab":{}},"source":["# Mean Absolute Error for Baseline Model\n","(predicted - true).abs().mean()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"laZIPZNg-K9r","colab_type":"text"},"source":["What if we just calculated \"Mean Error\", instead of \"Mean *Absolute* Error\"?"]},{"cell_type":"code","metadata":{"id":"KVUV_EQdl_YX","colab_type":"code","colab":{}},"source":["(predicted - true).mean()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tAN3TUE-P0u","colab_type":"text"},"source":["Positive and negative errors could zero each other out!"]},{"cell_type":"code","metadata":{"id":"iNI9jYNfl_YZ","colab_type":"code","colab":{}},"source":["-0.00000000000018"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPMonHpF-iQi","colab_type":"text"},"source":["This is why we do not use \"Mean Error.\" Instead, we use metrics like \"Mean Absolute Error\" or \"Mean Squared Error.\""]},{"cell_type":"markdown","metadata":{"id":"DTwbphKnl_Yc","colab_type":"text"},"source":["# More features"]},{"cell_type":"markdown","metadata":{"id":"IbfHfTjU-uQC","colab_type":"text"},"source":["The models so far have only used one feature, `carat`. What about `cut`?"]},{"cell_type":"code","metadata":{"id":"k35DQ_wdl_Yc","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CeUjvp4a-zmK","colab_type":"text"},"source":["If we try to use `cut` as-is, we get an error:"]},{"cell_type":"code","metadata":{"id":"JreD-w73l_Ye","colab_type":"code","colab":{}},"source":["features = ['carat', 'cut']\n","target = 'price'\n","model = LinearRegression()\n","model.fit(train[features], train[target])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwO88htk--kx","colab_type":"text"},"source":["Because `cut` is a string, not a number"]},{"cell_type":"code","metadata":{"id":"uQYzplA4l_Yj","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3s4HkfRl_Ym","colab_type":"text"},"source":["# Encode ordinal variable, from strings to numbers"]},{"cell_type":"markdown","metadata":{"id":"Df-e8SXn_DBp","colab_type":"text"},"source":["We can encode `cut` as a number instead of a string"]},{"cell_type":"code","metadata":{"id":"LgrVTFc-l_Yn","colab_type":"code","colab":{}},"source":["cut_ranks = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n","train.cut = train.cut.map(cut_ranks)\n","test.cut = test.cut.map(cut_ranks)\n","train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rh5HKpr5_OPq","colab_type":"text"},"source":["Now we can use `cut`."]},{"cell_type":"markdown","metadata":{"id":"WbEUjAXtl_Yo","colab_type":"text"},"source":["# Decision Tree"]},{"cell_type":"markdown","metadata":{"id":"ag8evGLk_Id7","colab_type":"text"},"source":["Let's try a new model type with our new feature."]},{"cell_type":"code","metadata":{"id":"pli124CRl_Yo","colab_type":"code","colab":{}},"source":["# Choose features and target\n","features = ['carat', 'cut']\n","target = 'price'\n","\n","# Fit model\n","from sklearn.tree import DecisionTreeRegressor\n","model = DecisionTreeRegressor(criterion='mae')\n","model.fit(train[features], train[target])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlpr_m4sl_Yq","colab_type":"code","colab":{}},"source":["def error():\n","    # Calculate mean absolute error on the training data\n","    y_true = train[target]\n","    y_pred = model.predict(train[features])\n","    train_error = mean_absolute_error(y_true, y_pred)\n","\n","    # Calculate mean absolute error on the test data\n","    y_true = test[target]\n","    y_pred = model.predict(test[features])\n","    test_error = mean_absolute_error(y_true, y_pred)\n","\n","    # Display the errors\n","    print('Train Error: $', round(train_error))\n","    print('Test Error: $', round(test_error))\n","    \n","error()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WO0IzKSX_Rcz","colab_type":"text"},"source":["The train error is zero! This model overfits."]},{"cell_type":"markdown","metadata":{"id":"upRVZ5V7l_Yv","colab_type":"text"},"source":["# Visualize the tree"]},{"cell_type":"markdown","metadata":{"id":"9jFX-Iv4_WD8","colab_type":"text"},"source":["Let's look at the tree to try to understand it better."]},{"cell_type":"markdown","metadata":{"id":"qvHmPlGZl_Yw","colab_type":"text"},"source":["#### To install Graphviz:\n","- On Google Colab, run the cell below.\n","- On your local machine, I recommend you use [Anaconda](https://www.anaconda.com/download/) instead: `conda install python-graphviz`"]},{"cell_type":"code","metadata":{"id":"G5zG3NU_l_Yw","colab_type":"code","colab":{}},"source":["!pip install graphviz\n","!apt-get install graphviz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2LtLG74l_Yy","colab_type":"code","colab":{}},"source":["import graphviz\n","from sklearn.tree import export_graphviz\n","\n","dot_data = export_graphviz(model, out_file=None, feature_names=features, filled=True, rotate=True)\n","graphviz.Source(dot_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Br2J9uSfl_Y1","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYwTQYGdl_Y3","colab_type":"text"},"source":["# Prune the tree"]},{"cell_type":"markdown","metadata":{"id":"w56HUz92_esd","colab_type":"text"},"source":["We can use the `min_samples_leaf` parameter to \"prune\" the tree and try to reduce overfitting."]},{"cell_type":"code","metadata":{"id":"GAnu-wJ6l_Y3","colab_type":"code","colab":{}},"source":["model = DecisionTreeRegressor(criterion='mae', min_samples_leaf=2)\n","model.fit(train[features], train[target])\n","error()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPxttYXD_qZt","colab_type":"text"},"source":["The tree is pruned, but the test error is even worse. 10 data points is probably not enough for this type of model."]},{"cell_type":"code","metadata":{"id":"cfhP3HU2l_Y5","colab_type":"code","colab":{}},"source":["dot_data = export_graphviz(model, out_file=None, feature_names=features, filled=True, rotate=True)\n","graphviz.Source(dot_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hk8EC7ETl_Y7","colab_type":"text"},"source":["# Multiple Regression"]},{"cell_type":"markdown","metadata":{"id":"_uVyNX7l_1B5","colab_type":"text"},"source":["Let's return to linear regression, our best model so far. This time, we'll use both features, to do a multiple regression."]},{"cell_type":"code","metadata":{"id":"CJG-asF0l_Y7","colab_type":"code","colab":{}},"source":["model = LinearRegression()\n","model.fit(train[features], train[target])\n","error()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BnAQr7JAFTQ","colab_type":"text"},"source":["This is our lowest test error so far. Here's the equation:"]},{"cell_type":"code","metadata":{"id":"kEITv59Dl_Y9","colab_type":"code","colab":{}},"source":["model.coef_, model.intercept_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqzFaxS_l_ZA","colab_type":"text"},"source":["$$ price = 4,771*carat + 82*cut - 1,449 $$"]},{"cell_type":"markdown","metadata":{"id":"GYG53ohDl_ZB","colab_type":"text"},"source":["### Predict the price of a 0.9 carat diamond with \"Very Good\" (3) cut "]},{"cell_type":"markdown","metadata":{"id":"Ji7ETje8l_ZB","colab_type":"text"},"source":["Using the formula:"]},{"cell_type":"code","metadata":{"id":"-HmiKeoSl_ZC","colab_type":"code","colab":{}},"source":["4771 * 0.9 + 82 * 3 - 1449"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RR_MZz0l_ZE","colab_type":"text"},"source":["The model's predict method gives the same result, as expected:"]},{"cell_type":"code","metadata":{"id":"A-Zrv98Ml_ZF","colab_type":"code","colab":{}},"source":["model.predict([[0.9, 3]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjpTYMtDl_ZH","colab_type":"text"},"source":["There's no comparable diamond in the training data ..."]},{"cell_type":"code","metadata":{"id":"WYOh_ukUl_ZH","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoG0e8GTl_ZK","colab_type":"text"},"source":["... but, the test data has two 0.9 carat diamonds. One has a better cut and a higher price. The other has a worse cut and a lower price. So the prediction looks plausible!"]},{"cell_type":"code","metadata":{"id":"XtJoSOL-l_ZK","colab_type":"code","colab":{}},"source":["test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lt08kGZRl_ZM","colab_type":"text"},"source":["# Could we figure out this formula, just by looking at the numbers?\n","\n","For the simple regression, sure. It's just twenty numbers. Plot the data and draw a line.\n","\n","But add just one more dimension, and it gets harder, even though it's still just thirty numbers."]},{"cell_type":"code","metadata":{"id":"AUyVnHmWl_ZM","colab_type":"code","colab":{}},"source":["train.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R429G1M6l_ZO","colab_type":"text"},"source":["And these thirty numbers come from a *much* bigger [dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) â€” which you'll use for your exercise!"]},{"cell_type":"code","metadata":{"id":"E-gtcHm5l_ZP","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","sns.load_dataset('diamonds').shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5jPRahwCM1O","colab_type":"text"},"source":["I wouldn't want to look at a half million numbers to search for patterns by hand. \n","\n","There's a fascinating story about [how a Russian mathematician constructed a decision tree - by hand - to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/).\n","\n","But I'm glad we have computers!"]},{"cell_type":"markdown","metadata":{"id":"w06pgtAMl_ZR","colab_type":"text"},"source":["# Recap, with references"]},{"cell_type":"markdown","metadata":{"id":"j9MSyoC2l_ZR","colab_type":"text"},"source":["### Google glossary\n","\n","#### [machine learning](https://developers.google.com/machine-learning/glossary/#machine_learning)\n","> A program or system that **builds (trains) a predictive model from input data**. The system uses the learned model **to make useful predictions from new (never-before-seen) data drawn from the same distribution** as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.\n","\n","#### [supervised machine learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)\n","> Training a model from input data and its corresponding labels. **Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers.** After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with unsupervised machine learning.\n","\n","#### [example](https://developers.google.com/machine-learning/glossary/#example)\n","> One row of a data set. **An example contains one or more features and** possibly **a label.** See also labeled example and unlabeled example.\n","\n","\n","#### [label](https://developers.google.com/machine-learning/glossary/#label)\n","> In supervised learning, the \"answer\" or \"result\" portion of an example. Each example in a labeled data set consists of one or more features and a label. **For instance, in a housing data set, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price.** In a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either \"spam\" or \"not spam.\"\n","\n","#### [target](https://developers.google.com/machine-learning/glossary/#target)\n","> Synonym for label.\n","\n","#### [regression model](https://developers.google.com/machine-learning/glossary/#regression_model)\n","> A type of model that outputs **continuous** (typically, floating-point) values. Compare with classification models, which output discrete values, such as \"day lily\" or \"tiger lily.\""]},{"cell_type":"markdown","metadata":{"id":"bRyCX_j-l_ZR","colab_type":"text"},"source":["### Wikipedia\n","\n","[**Machine learning**](https://en.wikipedia.org/wiki/Machine_learning) is a field of computer science that uses statistical techniques to **give computer systems the ability to \"learn\"** (e.g., progressively improve performance on a specific task) **with data, without being explicitly programmed.**\n","\n","The name machine learning was coined in 1959 by Arthur Samuel. Evolved from the study of **pattern recognition** and computational learning theory in **artificial intelligence**... Machine learning is employed in a range of computing tasks **where designing and programming explicit algorithms with good performance is difficult or infeasible**; example applications include email filtering, detection of network intruders, and computer vision.\n","\n","Machine learning is **closely related to (and often overlaps with) computational statistics**, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization ... \n","\n","Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to **\"produce reliable, repeatable decisions and results\"** and **uncover \"hidden insights\"** through learning from historical relationships and trends in the data.\n","\n","**Supervised learning:** The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that **maps inputs to outputs.**\n","\n","In **regression**, a supervised problem, the **outputs are continuous** rather than discrete."]},{"cell_type":"markdown","metadata":{"id":"idG-287bl_ZT","colab_type":"text"},"source":["### Scikit-learn documentation\n","- [Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html)\n","- [Linear Regression](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n","- [Polynomial Regression](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n","- [\"Dummy\" Regressor / Mean Baseline](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n","- [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"]},{"cell_type":"markdown","metadata":{"id":"UYamuAxgl_ZT","colab_type":"text"},"source":["### Free, practical books\n","- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/), Chapter 5, by Jake VanderPlas\n","- [The Mechanics of Machine Learning](https://mlbook.explained.ai/), by Terence Parr & Jeremy Howard"]}]}